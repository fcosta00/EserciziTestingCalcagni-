pazQ1 <- pazpreSD[pazpreSD$y>0,]
meanQ <- mean(pazQ1$visStop)
sdQ <- sd(pazQ1$visStop)
pazQ1$visStop <- replace(pazQ1$visStop, pazQ1$visStop > (meanQ + (sdQ*2.5)) | pazQ1$visStop < (meanQ - (sdQ*2.5)), meanQ)
meanQ <- mean(pazQ1$visStart)
sdQ <- sd(pazQ1$visStart)
pazQ1$visStart <- replace(pazQ1$visStart, pazQ1$visStart > (meanQ + (sdQ*2.5)) | pazQ1$visStart < (meanQ - (sdQ*2.5)), meanQ)
pazQ3 <- pazpreSD[pazpreSD$y <= 0,]
meanQ <- mean(pazQ3$visStop)
sdQ <- sd(pazQ3$visStop)
pazQ3$visStop <- replace(pazQ3$visStop, pazQ3$visStop > (meanQ + (sdQ*2.5)) | pazQ3$visStop < (meanQ - (sdQ*2.5)), meanQ)
meanQ <- mean(pazQ3$visStart)
sdQ <- sd(pazQ3$visStart)
pazpreSDnoOut <- rbind(pazQ3, pazQ1)
pazpreAvgNoOut <- data.frame( y= pazpreDSnoOut$y, visStart = (pazpreSDnoOut$visStart + pazpreDSnoOut$visStop)/2, visStop = (pazpreDSnoOut$visStart + pazpreSDnoOut$visStop)/2)
pazpreAvgNoOut$y
plotpreAvgNoOut <- ggplot() +  geom_path(data=pazpreAvgNoOut, aes(x=visStart, y=y, group=1)) + geom_point(data=pazpreAvgNoOut, aes(x=visStart, y=y, group=1), size=0.8) +
geom_path(data=pazpreAvgNoOut, aes(x=visStop, y=y, group=1)) + geom_point(data=pazpreAvgNoOut, aes(x=visStop, y=y, group=1), size=0.8)
plotpreAvgNoOut
summary(pazProcessed)
pazDelta <- ggplot() +  geom_path(data=pazpreAvgNoOut, aes(x=visStart, y=y, group=1.2), colour='darkred') + geom_point(data=pazpreAvgNoOut, aes(x=visStart, y=y, group=1), size=1.2, colour='darkred') +
geom_path(data=pazpreAvgNoOut, aes(x=visStop, y=y, group=1.2), colour='darkred') + geom_point(data=pazpreAvgNoOut, aes(x=visStop, y=y, group=1), size=1.2, colour='darkred')  +
geom_path(data=pazpostAvgNoOut, aes(x=visStart, y=y, group=1.2), colour='darkgreen') + geom_point(data=pazpostAvgNoOut, aes(x=visStart, y=y, group=1), size=1.2, colour='darkgreen') +
geom_path(data=pazpostAvgNoOut, aes(x=visStop, y=y, group=1.2), colour='darkgreen') + geom_point(data=pazpostAvgNoOut, aes(x=visStop, y=y, group=1), size=1.2, colour='darkgreen')  +
geom_text(aes(x=0, y=0),label='+', size=6, colour='red') +
scale_y_continuous(minor_breaks = seq(-960, 960, 40), breaks = seq(-520, 520, by = 80))  +
scale_x_continuous(minor_breaks = seq(-960, 960, 40), breaks = seq(-960, 960, by = 120), limits = c(-960, 960)) +
geom_vline(xintercept = -960, color = "blue", size=1) +
geom_vline(xintercept = 960, color = "blue", size=1) +
theme_minimal()
pazDelta
plotpostAvgNoOut
ggplot() +  geom_path(data=pazpostAvg, aes(x=visStart, y=y, group=1)) + geom_point(data=pazpostAvg, aes(x=visStart, y=y, group=1), size=0.8) +
geom_path(data=pazpostAvg, aes(x=visStop, y=y, group=1)) + geom_point(data=pazpostAvg, aes(x=visStop, y=y, group=1), size=0.8)
set.seed(123)
campione = rnorm(100)
t.test(campioone)
t.test(campione)
testEsempio = t.test(campione)
curve(dt(x, df=testEsempio$parameter), )
View(testEsempio)
curve(dt(x, df=testEsempio$parameter), from = -4, to = 4)
curve(dt(x, df=testEsempio$parameter), from = -4, to = 4, yla= t)
curve(dt(x, df=testEsempio$parameter), from = -4, to = 4, yla= 't')
abline(v= testEsempio$statistic, lty = 'dashed')
?dt
abline(v= testEsempio$statistic, lty = 'dashed', col="unicorn")
abline(v= testEsempio$statistic, lty = 'dashed', col="cacca")
abline(v= testEsempio$statistic, lty = 'dashed', col="poo")
abline(v= testEsempio$statistic, lty = 'dashed', col="pee")
install.packages("corrplot")
install.packages("corrplot")
plot(mpg ~ hp, data = mtcars)
mtcars_reg <- lm(mpg ~ hp, data = mtcars)
summary(mtcars_reg)
par(mfrow = c(3,1), mar = c(2,2,1,1)) #tre grafici in fila
{
# Retta di regressione
plot(mpg ~ hp, data = mtcars)
abline(mtcars_reg$coefficients, col = "red")
# Pattern nei residui
plot(mtcars_reg$residuals, main = "Residui")
# Distribuzione in quantili
qqnorm(mtcars_reg$residuals)
qqline(mtcars_reg$residuals)
plot(mtcars_reg, which = 1)
} #metodi verificare bontà modello
#prova regressione curva
mtcars_reg2 <- lm(mpg ~ hp + I(hp^2), data = mtcars)
summary(mtcars_reg2)
plot(mtcars_reg2, which = 1)
x = seq(min(mtcars$hp),
max(mtcars$hp),
length = 1000)
yhat = predict(mtcars_reg2, newdata = data.frame('hp'= x))
plot(mpg ~ hp, data = mtcars)
lines(x, yhat, col=2)
#es1
summary(mtcars)
plot(mpg ~ wt, data = mtcars)
mpgwt_reg <- lm(mpg  ~ wt, data = mtcars)
summary(mpgwt_reg)
plot(mpg  ~ wt, data = mtcars)
abline(mpgwt_reg$coefficients, col = "red")
plot(mpgwt_reg, which=1)
mpgwt_reg2 <- lm(mpg  ~ wt + I(wt^2), data = mtcars)
summary(mpgwt_reg2)
plot(mpgwt_reg2, which=1)
x = seq(min(mtcars$wt),
max(mtcars$wt),
length = 1000)
yhat = predict(mpgwt_reg2, newdata = data.frame('wt'= x))
plot(mpg ~ wt, data = mtcars)
lines(x, yhat, col=2)
#es regrex multipla
multi_reg <- lm(mpg  ~ wt + hp, data = mtcars)
summary(multi_reg)
#prova reg multipla sensata
multi_reg_pro <- lm(mpg  ~ ., data = mtcars)
multi_reg_pro <- lm(mpg  ~ wt * am * hp, data = mtcars)
summary(multi_reg_pro)
plot(mpg  ~ am, data= mtcars )
multi_reg <- lm(mpg  ~ wt + am + hp, data = mtcars)
summary(multi_reg)
} # https://thefreolo.github.io/book/regressione.html#regressione-multipla
# Esercizi e prove personali non presenti a lezione
{
plot(mpg ~ hp, data = mtcars)
mtcars_reg <- lm(mpg ~ hp, data = mtcars)
summary(mtcars_reg)
par(mfrow = c(3,1), mar = c(2,2,1,1)) #tre grafici in fila
{
# Retta di regressione
plot(mpg ~ hp, data = mtcars)
abline(mtcars_reg$coefficients, col = "red")
# Pattern nei residui
plot(mtcars_reg$residuals, main = "Residui")
# Distribuzione in quantili
qqnorm(mtcars_reg$residuals)
qqline(mtcars_reg$residuals)
plot(mtcars_reg, which = 1)
} #metodi verificare bontà modello
#prova regressione curva
mtcars_reg2 <- lm(mpg ~ hp + I(hp^2), data = mtcars)
summary(mtcars_reg2)
plot(mtcars_reg2, which = 1)
x = seq(min(mtcars$hp),
max(mtcars$hp),
length = 1000)
yhat = predict(mtcars_reg2, newdata = data.frame('hp'= x))
plot(mpg ~ hp, data = mtcars)
lines(x, yhat, col=2)
#es1
summary(mtcars)
plot(mpg ~ wt, data = mtcars)
mpgwt_reg <- lm(mpg  ~ wt, data = mtcars)
summary(mpgwt_reg)
plot(mpg  ~ wt, data = mtcars)
abline(mpgwt_reg$coefficients, col = "red")
plot(mpgwt_reg, which=1)
mpgwt_reg2 <- lm(mpg  ~ wt + I(wt^2), data = mtcars)
summary(mpgwt_reg2)
plot(mpgwt_reg2, which=1)
x = seq(min(mtcars$wt),
max(mtcars$wt),
length = 1000)
yhat = predict(mpgwt_reg2, newdata = data.frame('wt'= x))
plot(mpg ~ wt, data = mtcars)
lines(x, yhat, col=2)
#es regrex multipla
multi_reg <- lm(mpg  ~ wt + hp, data = mtcars)
summary(multi_reg)
#prova reg multipla sensata
multi_reg_pro <- lm(mpg  ~ ., data = mtcars)
multi_reg_pro <- lm(mpg  ~ wt * am * hp, data = mtcars)
summary(multi_reg_pro)
plot(mpg  ~ am, data= mtcars )
multi_reg <- lm(mpg  ~ wt + am + hp, data = mtcars)
summary(multi_reg)
} # https://thefreolo.github.io/book/regressione.html#regressione-multipla
# E S E R C I Z I O   1
{
summary(iris)
#stesso identi grafici, x(ascisse) -> Sepal.Length, y(ordinate) -> Petal.Width,
plot( iris$Sepal.Length, iris$Petal.Width )
plot( Petal.Width ~ Sepal.Length , data = iris)
#coloro il grafico in base alla specie
plot( iris$Sepal.Length, iris$Petal.Width, col = iris$Species )
plot( iris$Sepal.Length, iris$Petal.Width, pch = 20, col = c('red', 'blue', 'green')[iris$Species])
#la specie sembra essere indicativa in quanto nei due cluster le specie non sono mescolate ma son ben divise (nel cluster 1 è presente solo una specie)
# Modello senza Species
modNoSpecies = lm(Petal.Width ~ Sepal.Length, data=iris)
summary(modNoSpecies)
# Modello con Species
modNoInt = lm(Petal.Width ~ Sepal.Length + Species, data=iris)
summary(modNoInt)
# Modello con interazione Sepal.Length e Species (equivalente a dire Sepal.Length*Species, coefficiente angolare specifico)
modInt = lm(Petal.Width ~ Sepal.Length + Species + Sepal.Length:Species, data=iris)
summary(modInt)
library(sjPlot)
plot_model(modNoSpecies, type="pred", terms=c("Sepal.Length"), ci.lvl = NA)
plot_model(modNoInt, type="pred", terms=c("Sepal.Length", "Species"), ci.lvl = NA)
plot_model(modInt, type="pred", terms=c("Sepal.Length", "Species"), ci.lvl = NA)
#controllo se il modello di interazione è significativo rispetto a quello senza interazione
anova(modNoInt, modInt)
#valore del test F molto alto, accetto per cui l'ipotesi H0, il modello con interazione non è significativamente differente
anova(modInt) #test che mostra la devianza dovuta a ogni regressore inserito
}
# E S E R C I Z I O   2
{
library(MASS)
#funzioni per capire la struttura del database che stiamo analizzando
summary(anorexia)
head(anorexia)
#converto il peso da libbre a kg
anorexia$Prewt <- anorexia$Prewt*0.4535
anorexia$Postwt <- anorexia$Postwt*0.4535
#grafico per il confronto delle terapie pre e post
boxplot(anorexia$Prewt[anorexia$Treat == 'Cont'], anorexia$Postwt[anorexia$Treat == 'Cont'],
anorexia$Prewt[anorexia$Treat == 'CBT'], anorexia$Postwt[anorexia$Treat == 'CBT'],
anorexia$Prewt[anorexia$Treat == 'FT'], anorexia$Postwt[anorexia$Treat == 'FT'],
names = c("Cont Pre","Cont Post","CBT Pre", 'CBT Post', 'FT Pre', 'FT Post'),
col = c('cyan4', 'cyan3', 'coral4', 'coral3', 'darkolivegreen4', 'darkolivegreen3'),
ylab = 'Weigth')
#grafico per il confronto delle terapie con delta post pre
boxplot(Postwt-Prewt ~ Treat, data = anorexia,
col = c('coral3', 'cyan3', 'darkolivegreen3'),
ylab = 'Weigth')
mod_interazione <- lm(Postwt ~ Prewt, data = anorexia[anorexia$Treat == 'FT',])
plot(Postwt ~ Prewt, data = anorexia[anorexia$Treat == 'FT',])
anorexiere = anorexia
anorexia$Treat = relevel(anorexia$Treat, ref = "Cont")
mod = lm(Postwt-Prewt ~ 0 + Treat, data = anorexia)
mod = lm(Postwt-Prewt ~ Treat, data = anorexia)
summary(mod)
anorexiere$Treat <- relevel(anorexiere$Treat, "Cont")
mod <- lm(Postwt ~ Prewt + Treat + Prewt:Treat, data = anorexiere)
plot_model(mod, type="pred", terms=c("Prewt", "Treat"))
modCont <- lm(Postwt-Prewt ~ Prewt + Treat + Prewt:Treat, data = anorexia)
mod <- lm(Postwt-Prewt ~ Treat, data = anorexia)
modCont <- lm(Postwt-Prewt ~ Treat, data = anorexia[anorexia$Treat == 'Cont',])
summary(mod)
plot( iris$Sepal.Length, iris$Petal.Width )
plot( Postwt-Prewt ~ Treat , data = anorexia)
plot_model(mod, type="pred", terms=c("Treat"), ci.lvl = NA)
anova(modCont)
boxplot(Postwt-Prewt ~ Treat, data = anorexia)
}
library('corrplot')
install.packages("qgraph")
install.packages("clusterGeneration")
install.packages("psych")
install.packages("mvtnorm")
install.packages("lavaan")
install.packages("corrplot")
install.packages("ggraph")
#######################################################
### Testing psicologico (PSP6075525)
### A.A. 2022/2023
### prof. Antonio Calcagnì (antonio.calcagni@unipd.it)
#######################################################
## CONTENUTO DEL CODICE ##################################
# (A) Sintassi di lavaan: modello e stime
# (B) Sintassi lavaan: valutazione del modello
##########################################################
# Inizializzazione ambiente di lavoro -------------------------------------
rm(list=ls()); graphics.off()
setwd("~/MEGA/Lavoro_sync/Didattica/2022_2023/testing_psicologico/")
install.packages("lavaanPlot")
load("C:/Users/franc/Downloads/Datasets-20221124/covariance_wisc.Rda")
source('"C:\Users\franc\Downloads\Utilities-20221124\plot_lavaan_model.R"')
source("C:\Users\franc\Downloads\Utilities-20221124\plot_lavaan_model.R")
# E s e r c i z i o   2
{
library(datasets)
str(attitude)
summary(attitude)
att2 <- attitude[,2:7] #prendo solo le colonne da 2 a 7, escludo per cui la prima variabile oddervata
# 1° punto
describe(att2)
# 2° punto
source('Utilities-20221124/split_half.R')
sh <- split_half(att2)
#??? quanto è valido?
# 3° punto
punteggi = matrix(NA, 30,3)
punteggi[,1] <- rowSums(att2)
mediaGrezzo <- mean(punteggi[,1])
punteggi[,2] <- punteggi[,1]*sh + (1-sh)*mediaGrezzo
# 4° punto
library(psych)
summary(alpha(att2))
ac <- 0.81
punteggi[,3] <- punteggi[,1]*ac + (1-ac)*mediaGrezzo
colnames(punteggi) <- c('grezzo', 'split_half', 'cronbach')
# 5° punto
plot(density(punteggi[,1]), main = 'Grezzo', xlim=c(150,550))
plot(density(punteggi[,2]), main = 'Split Half', xlim=c(150,550))
plot(density(punteggi[,3]), main = 'Cronbach', xlim=c(150,550))
describe(punteggi)
# 6° punto
pairs(att2)
heatmap(cov(att2), scale='none')
library(corrplot)
corrplot(cor(att2))
#commento boh????????
}
library(psych)
# E s e r c i z i o   3
{
load('Datasets-20221124/mach/mach.Rdata')
# 1° punto
str(datax)
summary(datax)  # insieme di punteggi categoriali da -8 a 8 su 20 item divisi per nazione
# 2° punto
dataxITA <- datax[datax$country == "IT",]
dataxITA <- dataxITA[,1:20]
# 3° punto
library(lavaan)
heatmap(cor(dataxITA), scale = 'none')
mod_uni = ' lat1=~ Q6A + Q10A + Q7A + Q3A + Q9A + Q16A \n '
mod_uni_fit_UVI = cfa( model = mod_uni, data = dataxITA, std.lv = TRUE)
summary(mod_uni_fit_UVI, fit.measures = TRUE)
library(semPlot)
x11();semPaths(object = mod_uni_fit_UVI, what="model", whatLabels = "std")
# 4° punto
#* L'adattamento del modello sembra buono, i valori di lambda stimati oscillano tra i 0.44 e i 0.67
#* il RMSEA è molto basso 0.028 e inferiore al p-value < 0.05
# 5° punto
source('Utilities-20221124/reliability_semTools.R')
reliability(mod_uni_fit_UVI)
# 6° punto
mod_plu = ' lat1=~ Q6A + Q7A \n
lat2=~ Q4A + Q11A \n
lat3=~ Q1A + Q2A '
mod_plu_fit_UVI = cfa( model = mod_plu, data = dataxITA, std.lv = TRUE)
summary(mod_plu_fit_UVI, fit.measures = TRUE)
x11();semPaths(object = mod_plu_fit_UVI, what="model", whatLabels = "std")
# 7° piano
cfa_fits = matrix(NA, 2,6)  #creo mattice nuovo
cfa_fits[1,] = fitmeasures(object = mod_uni_fit_UVI,
fit.measures = c('RMSEA', 'CFI', 'AIC', 'chisq', 'df', 'npar'))
cfa_fits[2,] = fitmeasures(object = mod_plu_fit_UVI,
fit.measures = c('RMSEA', 'CFI', 'AIC', 'chisq', 'df', 'npar'))
colnames(cfa_fits) = c('RMSEA', 'CFI', 'AIC', 'chisq', 'df', 'npar')
rownames(cfa_fits) = c('uni', 'plu')
cfa_fits
reliability(mod_plu_fit_UVI)
}
setwd("C:/Users/franc/OneDrive - Università degli Studi di Padova/Università-PC-senzaMilza/3° Anno/Testing/EserciziTestingCalcagni-")
load('Datasets-20221124/data_ex4.csv')
data_ex4 <- read.csv("C:/Users/franc/OneDrive - Università degli Studi di Padova/Università-PC-senzaMilza/3° Anno/Testing/EserciziTestingCalcagni-/Datasets-20221124/data_ex4.csv")
View(data_ex4)
str(data_ex4)
# 1° punto
mod1_uni <- 'lat1=~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15 \n
}
# 1° punto
mod1_uni <- 'lat1=~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15 \n'
# 1° punto
mod1_uni <- 'lat1=~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15 \n'
library(lavaan)
mod1_uni_fit <- cfa(model = mod1_uni, data = data_ex4)
mod1_uni_fit <- cfa(model = mod1_uni, data = data_ex4, ordered = names(data_ex4), estimator="DWLS")
summary(mod1_uni_fit, fit.measures = TRUE, standardized=TRUE )
# 1° punto
data_ord = data_ex4 # d'ora innanzi lavoriamo su bfi.ord che è lo stesso di bfi_B e contiene variabili dichiarate come ordinali
for(j in 1:NCOL(data_ord)){
data_ord[,j] = factor(data_ord[,j],ordered = TRUE)
}
mod1_uni <- 'lat1=~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15 \n'
mod1_uni_fit <- cfa(model = mod1_uni, data = data_ex4, ordered = names(data_ex4), estimator="DWLS")
summary(mod1_uni_fit, fit.measures = TRUE, standardized=TRUE )
summary(mod_tri_ort_fit, fit.measures = TRUE, standardized=TRUE )
mod_tri_ort <- ' lat1=~ V1+V2+V3+V4+V5 \n
lat2=~ V6+V7+V8+V9+V10 \n
lat3=~ V11+V12+V13+V14+V15 \n
lat1~~ 0*lat2 \n
lat1~~ 0*lat3 \n
lat2~~ 0*lat3 \n'
mod_tri_ort_fit <- cfa(model = mod_tri_ort, data = data_ex4, ordered = names(data_ex4), estimator="DWLS")
summary(mod_tri_ort_fit, fit.measures = TRUE, standardized=TRUE )
mod_uni <- 'lat1=~ V1+V2+V3+V4+V5+V6+V7+V8+V9+V10+V11+V12+V13+V14+V15 \n'
mod_uni_fit <- cfa(model = mod1_uni, data = data_ord, ordered = names(data_ord), estimator="DWLS")
summary(mod1_uni_fit, fit.measures = TRUE, standardized=TRUE )
summary(data_ord)
# 2° punto
mod_tri_ort <- ' lat1=~ V1+V2+V3+V4+V5 \n
lat2=~ V6+V7+V8+V9+V10 \n
lat3=~ V11+V12+V13+V14+V15 \n
lat1~~ 0*lat2 \n
lat1~~ 0*lat3 \n
lat2~~ 0*lat3 \n'
mod_tri_ort_fit <- cfa(model = mod_tri_ort, data = data_ord, ordered = names(data_ord), estimator="DWLS")
summary(mod_tri_ort_fit, fit.measures = TRUE, standardized=TRUE )
bfi.fits = matrix(NA,2,5) #matrice per i risultati dei fit dei modelli
bfi.fits[1,] = fitmeasures(object = mod_uni_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = mod_tri_ort_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
rownames(bfi.fits) = c("mod_uni","mod_tri")
View(bfi.fits)
colnames(bfi.fits) = c("RMSEA","CFI","chisq","df","npar")
mod_tri_clx <- ' lat1=~ V1+V2+V3+V4+V5 \n
lat2=~ V6+V7+V8+V9+V10 \n
lat3=~ V11+V12+V13+V14+V15 \n'
mod_tri_clx_fit <- cfa(model = mod_tri_clx, data = data_ord, ordered = names(data_ord), estimator="DWLS")
summary(mod_tri_clx_fit, fit.measures = TRUE, standardized=TRUE )
# 3° punto
bfi.fits = matrix(NA,3,5) #matrice per i risultati dei fit dei modelli
bfi.fits[1,] = fitmeasures(object = mod_uni_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = mod_tri_ort_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = mod_tri_clx_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
rownames(bfi.fits) = c("mod_uni","mod_tri_ort",'mod_tri_clx')
colnames(bfi.fits) = c("RMSEA","CFI","chisq","df","npar")
#il mod_tri è migliore secondo ogni indice
View(bfi.fits)
# 3° punto
bfi.fits = matrix(NA,3,5) #matrice per i risultati dei fit dei modelli
bfi.fits[1,] = fitmeasures(object = mod_uni_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = mod_tri_ort_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[3,] = fitmeasures(object = mod_tri_clx_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
rownames(bfi.fits) = c("mod_uni","mod_tri_ort",'mod_tri_clx')
colnames(bfi.fits) = c("RMSEA","CFI","chisq","df","npar")
#il mod_tri è migliore secondo ogni indice
library(semPlot)
# 5° punto
plot_lavaan_model(fitted_model = bfi.cfa0,intercepts=FALSE,)
source('Utilities-20221124/plot_lavaan_model.R')
plot_lavaan_model(fitted_model = bfi.cfa0,intercepts=FALSE,)
# 5° punto
plot_lavaan_model(fitted_model = bfi.cfa0,intercepts=FALSE)
# 5° punto
plot_lavaan_model(fitted_model = bfi.cfa0)
# 5° punto
plot_lavaan_model(fitted_model = mod_tri_clx_fit,intercepts=FALSE)
# 5° punto
plot_lavaan_model(fitted_model = mod_tri_clx_fit,intercepts=FALSE,)
# 5° punto
plot_lavaan_model(fitted_model = mod_tri_clx_fi)
# 5° punto
plot_lavaan_model(fitted_model = mod_tri_clx_fit)
# Estrazione delle matrici del modello
A = inspect(object = bfi.cfa0,what = "std.all")
# Estrazione delle matrici del modello
A = inspect(object = mod_tri_clx_fit,what = "std.all")
A$lambda #Lambda
A$theta #Theta_delta
A$psi #Phi
# 6° punto
data_ord_cor <- cor( data_ord, method ='spearman') #spearman perchè sono categoriali
corrplot(corr = data_ord_cor, method = 'circle')
# 6° punto
bfi.eta = lavPredict(object = mod_tri_clx_fit,type = "lv",method = "regression")
View(bfi.eta)
# 6° punto
cor_ex4 <- cor( data_ex4, method ='spearman')
data_ex4_hclust = hclust( d= dist(cor_ex4), method = 'ward.D2')
plot(data_ex4_hclust);
data_ex4_hclust = cutree(tree = data_ex4_hclust, k = 3)
names(data_ex4_hclust[data_ex4_hclust==1])
names(data_ex4_hclust[data_ex4_hclust==2])
names(data_ex4_hclust[data_ex4_hclust==3])
data_ex4_hclust = hclust( d= dist(cor_ex4), method = 'complete')
plot(data_ex4_hclust);
mod_tri_ort_ward_fit <- cfa(model = mod_tri_ort_ward, data = data_ord, ordered = names(data_ord), estimator="DWLS")
mod_tri_ort_ward <- 'lat1=~ V2+V12+V14+V9+V13+V3+V11+V15 \n
lat2=~ V1+V4+V5 \n
lat3=~ V6+V7+V8+V10 \n
lat1~~ 0*lat2 \n
lat1~~ 0*lat3 \n
lat2~~ 0*lat3 \n'
mod_tri_ort_full <- 'lat1=~ V2+V12+V14+V9+V13+V3+V11+V15 \n
lat2=~ V1+V4+V5+V6+V7+V8+V10 \n
lat1~~ 0*lat2 \n'
mod_tri_ort_ward_fit <- cfa(model = mod_tri_ort_ward, data = data_ord, ordered = names(data_ord), estimator="DWLS")
mod_tri_ort_full_fit <- cfa(model = mod_tri_ort_full, data = data_ord, ordered = names(data_ord), estimator="DWLS")
bfi.fits = matrix(NA,3,5) #matrice per i risultati dei fit dei modelli
bfi.fits[1,] = fitmeasures(object = mod_tri_ort_ward_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = mod_tri_ort_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[3,] = fitmeasures(object = mod_tri_ort_full_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
rownames(bfi.fits) = c("ward","mod_tri_ort",'complete')
colnames(bfi.fits) = c("RMSEA","CFI","chisq","df","npar")
View(bfi.fits)
mod_tri_ort_ward <- 'lat1=~ V2+V12+V14+V9+V13+V3+V11+V15 \n
lat2=~ V1+V4+V5 \n
lat3=~ V6+V7+V8+V10 \n'
mod_tri_ort_full <- 'lat1=~ V2+V12+V14+V9+V13+V3+V11+V15 \n
lat2=~ V1+V4+V5+V6+V7+V8+V10 \n'
mod_tri_ort_ward_fit <- cfa(model = mod_tri_ort_ward, data = data_ord, ordered = names(data_ord), estimator="DWLS")
mod_tri_ort_full_fit <- cfa(model = mod_tri_ort_full, data = data_ord, ordered = names(data_ord), estimator="DWLS")
bfi.fits = matrix(NA,3,5) #matrice per i risultati dei fit dei modelli
bfi.fits[1,] = fitmeasures(object = mod_tri_ort_ward_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = mod_tri_ort_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[3,] = fitmeasures(object = mod_tri_ort_full_fit,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
rownames(bfi.fits) = c("ward","mod_tri_ort",'complete')
colnames(bfi.fits) = c("RMSEA","CFI","chisq","df","npar")
View(bfi.fits)
A$theta #Theta_delta
library(lavaan)
data('HolzingerSwineford1939')
summary(HolzingerSwineford1939)
str(HolzingerSwineford1939)
#  =~ simbolo lavaan che indica legame tra latente e osservate
#  +  indica la concatenazione, non la somma
mod_visual <- " percezione =~ x1 + x2 + x3 "
mod_visual_fit <- cfa(model = mod_visual, data = HolzingerSwineford1939)
# UTILIZZATA METRICA ULI
# estimate(latent) = lambda
# estimate(variances) = teta delta
# estimate(percezione) = fi 11
summary(mod_visual_fit)
summary(mod_visual_fit, standardized = TRUE) #ci interessa std.all (sup o.30 solitamente si interpretano, meno che se fotte)
library(semPlot)
semPaths(object= mod_visual_fit, what= 'model', whatLabels = 'est')
mod_visual <- " percezione =~ x1 + x2 + x3 "
mod_visual_fit <- cfa(model = mod_visual, data = HolzingerSwineford1939, std.lv = TRUE) #standardizza secondo uli (FALSE UVI, TRUE ULI)
#usare solo la matrice di covarianza per fare una cfa con lavaan
Sy = cov(HolzingerSwineford1939[,7:15])
print(Sy)
apply(X = HolzingerSwineford1939[,7:15], MARGIN = 2, FUN = mean)  #utilizzare apply per lavorare a colonne (MARGIN=2->columnwise, MARGIN=1->rowwise)
hz_std = scale(x = HolzingerSwineford1939[,7:15], center = TRUE, scale = FALSE)  #centrare variabili per assunzione che tau = 0
apply(X = hz_std, MARGIN = 2, FUN = mean) #ora le medie tendono a zero
apply(X = hz_std, MARGIN = 2, FUN = var) #var diversa da 1 perche non abbiamo fatto la scalatura( con solo centratura la varianza non ha subito modifiche)
#scalatura divide per varianza mentre centratura sottrae la media
Sy = cov(hz_std)
mod_visual_fit<-cfa(model = mod_visual, sample.cov = Sy, sample.nobs = 301) #necessiamo della numerosità -> z-value è il valore della statistica utilizzata per fare inferenza sui lamba
# per calcolare lo std error ci serve la numerosità che a sua volta ci serve per la distribuzione z
model_visual_stimato = inspect( object = mod_visual_fit, what= 'est') #con what = std  standardizza le matrici stimate
print(model_visual_stimato)
source("C:/Users/franc/OneDrive - Università degli Studi di Padova/Università-PC-senzaMilza/3° Anno/Testing/EserciziTestingCalcagni-/lab1_appunti.R", echo=TRUE)
Theta_delta = model_visual_stimato$theta #varianza residui mentre varianza degli errori 0
View(Theta_delta)
library(psych)
