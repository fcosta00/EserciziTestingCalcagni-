plot(mpg ~ wt, data = mtcars)
mpgwt_reg <- lm(mpg  ~ wt, data = mtcars)
summary(mpgwt_reg)
plot(mpg  ~ wt, data = mtcars)
abline(mpgwt_reg$coefficients, col = "red")
plot(mpgwt_reg, which=1)
mpgwt_reg2 <- lm(mpg  ~ wt + I(wt^2), data = mtcars)
summary(mpgwt_reg2)
plot(mpgwt_reg2, which=1)
x = seq(min(mtcars$wt),
max(mtcars$wt),
length = 1000)
yhat = predict(mpgwt_reg2, newdata = data.frame('wt'= x))
plot(mpg ~ wt, data = mtcars)
lines(x, yhat, col=2)
#es regrex multipla
multi_reg <- lm(mpg  ~ wt + hp, data = mtcars)
summary(multi_reg)
#prova reg multipla sensata
multi_reg_pro <- lm(mpg  ~ ., data = mtcars)
multi_reg_pro <- lm(mpg  ~ wt * am * hp, data = mtcars)
summary(multi_reg_pro)
plot(mpg  ~ am, data= mtcars )
multi_reg <- lm(mpg  ~ wt + am + hp, data = mtcars)
summary(multi_reg)
} # https://thefreolo.github.io/book/regressione.html#regressione-multipla
# Esercizi e prove personali non presenti a lezione
{
plot(mpg ~ hp, data = mtcars)
mtcars_reg <- lm(mpg ~ hp, data = mtcars)
summary(mtcars_reg)
par(mfrow = c(3,1), mar = c(2,2,1,1)) #tre grafici in fila
{
# Retta di regressione
plot(mpg ~ hp, data = mtcars)
abline(mtcars_reg$coefficients, col = "red")
# Pattern nei residui
plot(mtcars_reg$residuals, main = "Residui")
# Distribuzione in quantili
qqnorm(mtcars_reg$residuals)
qqline(mtcars_reg$residuals)
plot(mtcars_reg, which = 1)
} #metodi verificare bontà modello
#prova regressione curva
mtcars_reg2 <- lm(mpg ~ hp + I(hp^2), data = mtcars)
summary(mtcars_reg2)
plot(mtcars_reg2, which = 1)
x = seq(min(mtcars$hp),
max(mtcars$hp),
length = 1000)
yhat = predict(mtcars_reg2, newdata = data.frame('hp'= x))
plot(mpg ~ hp, data = mtcars)
lines(x, yhat, col=2)
#es1
summary(mtcars)
plot(mpg ~ wt, data = mtcars)
mpgwt_reg <- lm(mpg  ~ wt, data = mtcars)
summary(mpgwt_reg)
plot(mpg  ~ wt, data = mtcars)
abline(mpgwt_reg$coefficients, col = "red")
plot(mpgwt_reg, which=1)
mpgwt_reg2 <- lm(mpg  ~ wt + I(wt^2), data = mtcars)
summary(mpgwt_reg2)
plot(mpgwt_reg2, which=1)
x = seq(min(mtcars$wt),
max(mtcars$wt),
length = 1000)
yhat = predict(mpgwt_reg2, newdata = data.frame('wt'= x))
plot(mpg ~ wt, data = mtcars)
lines(x, yhat, col=2)
#es regrex multipla
multi_reg <- lm(mpg  ~ wt + hp, data = mtcars)
summary(multi_reg)
#prova reg multipla sensata
multi_reg_pro <- lm(mpg  ~ ., data = mtcars)
multi_reg_pro <- lm(mpg  ~ wt * am * hp, data = mtcars)
summary(multi_reg_pro)
plot(mpg  ~ am, data= mtcars )
multi_reg <- lm(mpg  ~ wt + am + hp, data = mtcars)
summary(multi_reg)
} # https://thefreolo.github.io/book/regressione.html#regressione-multipla
# E S E R C I Z I O   1
{
summary(iris)
#stesso identi grafici, x(ascisse) -> Sepal.Length, y(ordinate) -> Petal.Width,
plot( iris$Sepal.Length, iris$Petal.Width )
plot( Petal.Width ~ Sepal.Length , data = iris)
#coloro il grafico in base alla specie
plot( iris$Sepal.Length, iris$Petal.Width, col = iris$Species )
plot( iris$Sepal.Length, iris$Petal.Width, pch = 20, col = c('red', 'blue', 'green')[iris$Species])
#la specie sembra essere indicativa in quanto nei due cluster le specie non sono mescolate ma son ben divise (nel cluster 1 è presente solo una specie)
# Modello senza Species
modNoSpecies = lm(Petal.Width ~ Sepal.Length, data=iris)
summary(modNoSpecies)
# Modello con Species
modNoInt = lm(Petal.Width ~ Sepal.Length + Species, data=iris)
summary(modNoInt)
# Modello con interazione Sepal.Length e Species (equivalente a dire Sepal.Length*Species, coefficiente angolare specifico)
modInt = lm(Petal.Width ~ Sepal.Length + Species + Sepal.Length:Species, data=iris)
summary(modInt)
library(sjPlot)
plot_model(modNoSpecies, type="pred", terms=c("Sepal.Length"), ci.lvl = NA)
plot_model(modNoInt, type="pred", terms=c("Sepal.Length", "Species"), ci.lvl = NA)
plot_model(modInt, type="pred", terms=c("Sepal.Length", "Species"), ci.lvl = NA)
#controllo se il modello di interazione è significativo rispetto a quello senza interazione
anova(modNoInt, modInt)
#valore del test F molto alto, accetto per cui l'ipotesi H0, il modello con interazione non è significativamente differente
anova(modInt) #test che mostra la devianza dovuta a ogni regressore inserito
}
# E S E R C I Z I O   2
{
library(MASS)
#funzioni per capire la struttura del database che stiamo analizzando
summary(anorexia)
head(anorexia)
#converto il peso da libbre a kg
anorexia$Prewt <- anorexia$Prewt*0.4535
anorexia$Postwt <- anorexia$Postwt*0.4535
#grafico per il confronto delle terapie pre e post
boxplot(anorexia$Prewt[anorexia$Treat == 'Cont'], anorexia$Postwt[anorexia$Treat == 'Cont'],
anorexia$Prewt[anorexia$Treat == 'CBT'], anorexia$Postwt[anorexia$Treat == 'CBT'],
anorexia$Prewt[anorexia$Treat == 'FT'], anorexia$Postwt[anorexia$Treat == 'FT'],
names = c("Cont Pre","Cont Post","CBT Pre", 'CBT Post', 'FT Pre', 'FT Post'),
col = c('cyan4', 'cyan3', 'coral4', 'coral3', 'darkolivegreen4', 'darkolivegreen3'),
ylab = 'Weigth')
#grafico per il confronto delle terapie con delta post pre
boxplot(Postwt-Prewt ~ Treat, data = anorexia,
col = c('coral3', 'cyan3', 'darkolivegreen3'),
ylab = 'Weigth')
mod_interazione <- lm(Postwt ~ Prewt, data = anorexia[anorexia$Treat == 'FT',])
plot(Postwt ~ Prewt, data = anorexia[anorexia$Treat == 'FT',])
anorexiere = anorexia
anorexia$Treat = relevel(anorexia$Treat, ref = "Cont")
mod = lm(Postwt-Prewt ~ 0 + Treat, data = anorexia)
mod = lm(Postwt-Prewt ~ Treat, data = anorexia)
summary(mod)
anorexiere$Treat <- relevel(anorexiere$Treat, "Cont")
mod <- lm(Postwt ~ Prewt + Treat + Prewt:Treat, data = anorexiere)
plot_model(mod, type="pred", terms=c("Prewt", "Treat"))
modCont <- lm(Postwt-Prewt ~ Prewt + Treat + Prewt:Treat, data = anorexia)
mod <- lm(Postwt-Prewt ~ Treat, data = anorexia)
modCont <- lm(Postwt-Prewt ~ Treat, data = anorexia[anorexia$Treat == 'Cont',])
summary(mod)
plot( iris$Sepal.Length, iris$Petal.Width )
plot( Postwt-Prewt ~ Treat , data = anorexia)
plot_model(mod, type="pred", terms=c("Treat"), ci.lvl = NA)
anova(modCont)
boxplot(Postwt-Prewt ~ Treat, data = anorexia)
}
library('corrplot')
install.packages("qgraph")
install.packages("clusterGeneration")
install.packages("psych")
install.packages("mvtnorm")
install.packages("lavaan")
install.packages("corrplot")
install.packages("ggraph")
#######################################################
### Testing psicologico (PSP6075525)
### A.A. 2022/2023
### prof. Antonio Calcagnì (antonio.calcagni@unipd.it)
#######################################################
## CONTENUTO DEL CODICE ##################################
# (A) Sintassi di lavaan: modello e stime
# (B) Sintassi lavaan: valutazione del modello
##########################################################
# Inizializzazione ambiente di lavoro -------------------------------------
rm(list=ls()); graphics.off()
setwd("~/MEGA/Lavoro_sync/Didattica/2022_2023/testing_psicologico/")
install.packages("lavaanPlot")
load("C:/Users/franc/Downloads/Datasets-20221124/covariance_wisc.Rda")
source('"C:\Users\franc\Downloads\Utilities-20221124\plot_lavaan_model.R"')
source("C:\Users\franc\Downloads\Utilities-20221124\plot_lavaan_model.R")
# E s e r c i z i o   2
{
library(datasets)
str(attitude)
summary(attitude)
att2 <- attitude[,2:7] #prendo solo le colonne da 2 a 7, escludo per cui la prima variabile oddervata
# 1° punto
describe(att2)
# 2° punto
source('Utilities-20221124/split_half.R')
sh <- split_half(att2)
#??? quanto è valido?
# 3° punto
punteggi = matrix(NA, 30,3)
punteggi[,1] <- rowSums(att2)
mediaGrezzo <- mean(punteggi[,1])
punteggi[,2] <- punteggi[,1]*sh + (1-sh)*mediaGrezzo
# 4° punto
library(psych)
summary(alpha(att2))
ac <- 0.81
punteggi[,3] <- punteggi[,1]*ac + (1-ac)*mediaGrezzo
colnames(punteggi) <- c('grezzo', 'split_half', 'cronbach')
# 5° punto
plot(density(punteggi[,1]), main = 'Grezzo', xlim=c(150,550))
plot(density(punteggi[,2]), main = 'Split Half', xlim=c(150,550))
plot(density(punteggi[,3]), main = 'Cronbach', xlim=c(150,550))
describe(punteggi)
# 6° punto
pairs(att2)
heatmap(cov(att2), scale='none')
library(corrplot)
corrplot(cor(att2))
#commento boh????????
}
library(psych)
# E s e r c i z i o   3
{
load('Datasets-20221124/mach/mach.Rdata')
# 1° punto
str(datax)
summary(datax)  # insieme di punteggi categoriali da -8 a 8 su 20 item divisi per nazione
# 2° punto
dataxITA <- datax[datax$country == "IT",]
dataxITA <- dataxITA[,1:20]
# 3° punto
library(lavaan)
heatmap(cor(dataxITA), scale = 'none')
mod_uni = ' lat1=~ Q6A + Q10A + Q7A + Q3A + Q9A + Q16A \n '
mod_uni_fit_UVI = cfa( model = mod_uni, data = dataxITA, std.lv = TRUE)
summary(mod_uni_fit_UVI, fit.measures = TRUE)
library(semPlot)
x11();semPaths(object = mod_uni_fit_UVI, what="model", whatLabels = "std")
# 4° punto
#* L'adattamento del modello sembra buono, i valori di lambda stimati oscillano tra i 0.44 e i 0.67
#* il RMSEA è molto basso 0.028 e inferiore al p-value < 0.05
# 5° punto
source('Utilities-20221124/reliability_semTools.R')
reliability(mod_uni_fit_UVI)
# 6° punto
mod_plu = ' lat1=~ Q6A + Q7A \n
lat2=~ Q4A + Q11A \n
lat3=~ Q1A + Q2A '
mod_plu_fit_UVI = cfa( model = mod_plu, data = dataxITA, std.lv = TRUE)
summary(mod_plu_fit_UVI, fit.measures = TRUE)
x11();semPaths(object = mod_plu_fit_UVI, what="model", whatLabels = "std")
# 7° piano
cfa_fits = matrix(NA, 2,6)  #creo mattice nuovo
cfa_fits[1,] = fitmeasures(object = mod_uni_fit_UVI,
fit.measures = c('RMSEA', 'CFI', 'AIC', 'chisq', 'df', 'npar'))
cfa_fits[2,] = fitmeasures(object = mod_plu_fit_UVI,
fit.measures = c('RMSEA', 'CFI', 'AIC', 'chisq', 'df', 'npar'))
colnames(cfa_fits) = c('RMSEA', 'CFI', 'AIC', 'chisq', 'df', 'npar')
rownames(cfa_fits) = c('uni', 'plu')
cfa_fits
reliability(mod_plu_fit_UVI)
}
setwd("C:/Users/franc/OneDrive - Università degli Studi di Padova/Università-PC-senzaMilza/3° Anno/Testing/EserciziTestingCalcagni-")
load('Datasets-20221124/wisc_simdata.Rdata')
# Prima di cominciare lo studio dell'invarianza, trasformiamo la variabile "group" da fattore a 2 livelli in variabile stringa (come vuole lavaan).
wisc$group = as.character(wisc$group)
# Scriviamo anche il modello generale della WISC-IV secondo la sintassi di lavaan:
model.wisc = "VCI=~SO+VC+CO \n PRI=~DC+CI+RM \n WMI=~MC+LN \n PSI=~CR+RS"
# 1) invarianza configurale
cfa.wisc.configural = cfa(model = model.wisc,data = wisc,group = "group")
# 2) invarianza debole
cfa.wisc.weak = cfa(model = model.wisc,data = wisc,group = "group",group.equal="loadings")
library(lavaan)
library(semPlot)
library(mvtnorm)
# 2) invarianza debole
cfa.wisc.weak = cfa(model = model.wisc,data = wisc,group = "group",group.equal="loadings")
# 1) invarianza configurale
cfa.wisc.configural = cfa(model = model.wisc,data = wisc,group = "group")
# 2) invarianza debole
cfa.wisc.weak = cfa(model = model.wisc,data = wisc,group = "group",group.equal="loadings")
anova(cfa.wisc.configural,cfa.wisc.weak)
# 3) invarianza forte
cfa.wisc.strong = cfa(model = model.wisc,data = wisc,group = "group",group.equal=c("loadings","intercepts"))
anova(cfa.wisc.weak,cfa.wisc.strong)
# 4) invarianza esatta
cfa.wisc.strict = cfa(model = model.wisc,data = wisc,group = "group",group.equal=c("loadings","intercepts","residuals"))
anova(cfa.wisc.strong,cfa.wisc.strict)
fitmeasures(object = cfa.wisc.strong,fit.measures = c("RMSEA","CFI","chisq","df","npar","AIC"))
semTools::reliability(cfa.wisc.strong)
summary(cfa.wisc.strong,standardized=TRUE)
summary(cfa.wisc.strong,standardized=TRUE)
load('Datasets-20221124/ISI.Rdata')
Y = psych::rescale(x = Y,mean = 0,sd = 1) #standardizziamo i dati iniziali
# Two-factor model:
## Cit. Sierra, J. C., Guillén-Serrano, V., & Santos-Iglesias, P. (2008). Insomnia Severity Index: some indicators about its reliability and validity on an older adults sample. Revista de neurologia, 47(11), 566-570.
# The first factor included items assessing severity of sleeping difficulties. The items covered difficulties in initiating and maintaining sleep, as well as satisfaction with current sleeping pattern.
# The second factor focused more on the impact of insomnia, and included items assessing day time interference and distress associated with insomnia, as well as how noticeable the sleeping problem was.
isi.twof = "
sleep_diff =~ wakeup_early+staying_asleep+falling_asleep+satisfied
impact =~ worried+noticeable+inference
"
isi.twof_fit = cfa(model = isi.twof,data = Y)
summary(isi.twof_fit,standardize=TRUE)
x11(); semPaths(object = isi.twof_fit,what="model",whatLabels = "std.all",style="lisrel",nCharNodes = 3,edge.label.cex = 1.25,sizeMan=5,sizeLat = 6,nDigits = 2,title = TRUE)
# Three-factor model:
## Fernandez-Mendoza, J., Rodriguez-Muñoz, A., Vela-Bueno, A., Olavarrieta-Bernardino, S., Calhoun, S. L., Bixler, E. O., & Vgontzas, A. N. (2012). The Spanish version of the Insomnia Severity Index: a confirmatory factor analysis. Sleep medicine, 13(2), 207-210.
#The first factor was termed the impact component and defined by items referring to distress, interference, and noticeability of insomnia.
#The second factor was named the severity component, as it contained three items describing the primary symptoms of insomnia at different time points of the night (initial, middle, and terminal).
#The third factor was called the (dis)satisfaction component and was defined by a single item about overall satisfaction with sleep.
#According to Bastien et al., these three factors captured the main diagnostic criteria for insomnia.
isi.threef = "
sleep_diff =~ wakeup_early+staying_asleep+falling_asleep
dissatisf =~ satisfied+worried+wakeup_early
impact =~ worried+noticeable+inference
"
isi.threef_fit = cfa(model = isi.threef,data = Y)
summary(isi.threef_fit,standardize=TRUE)
x11(); semPaths(object = isi.threef_fit,what="model",whatLabels = "std.all",style="lisrel",nCharNodes = 3,edge.label.cex = 1.25,sizeMan=5,sizeLat = 6,nDigits = 2)
# High-order factor model (second-level measurement model):
## More info at: https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01357/full#h4
# The higher-order model (Thurstone, 1944) incorporates at least one superordinate (higher-order) factor and a series of subordinate factors upon which specified sub-group of items load.
# The higher-order model estimates two sets of loadings: those showing the relationships between the observed variables and the relevant grouping, or subordinate, factor, plus those showing the relationship between the higher-order factor and each of the subordinate factors.
# Higher-order models are often used for theory testing (Brown, 2015), and they enable the researcher to explore theoretical understandings of the relationship between a series of sub-tests as distinct from one another, but also united by a common factor, which attempts to explain the scores in the higher-order factor.
# If the loadings between the higher-order and subordinate factors are satisfactorily high, it can be concluded that there is enough commonality between the sub-skills to justify this reporting both sub-scores and an overall score.
# It is important to note that in this model the observed variables act as indicators of the subordinate factors, and therefore, the commonality modeled by the higher-order factor is between the scales already established for each sub-group. This mediating role for the subordinate factors means that the higher-order factor, therefore, represents a “distilled” estimate of general ability rather than a more direct estimate, which accounts for commonalities between all observed variables as per the unidimensional model.
isi.so = "
sleep_diff =~ wakeup_early+staying_asleep+falling_asleep
dissatisf =~ satisfied+worried+wakeup_early
impact =~ worried+noticeable+inference
eta =~ sleep_diff+dissatisf+impact
"
# Bifactor structure
## More info at: https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01357/full#h4
# The bifactor model (Holzinger and Swineford, 1937), also described as a nested-factor (NF) model (Gustafsson and Åberg-Bengtsson, 2010; Brunner et al., 2012), incorporates a general factor, which loads directly onto all of the observed variables in the model.
# One of the defining features of the bifactor model is that the grouping factors in the model are hypothesized to be orthogonal (uncorrelated) with the general factor.
# The bifactor model does not offer a “simple structure” solution in which each observed variable only loads onto a single factor (Gustafsson and Åberg-Bengtsson, 2010). Observed variables, by design, in this model load onto more than one factor, meaning that the variance explanation is split between (at least) two latencies. Each observed variable in the bifactor model is an indicator of both the general factor and one grouping factor. This means that each observed variable has two loading estimates in the model; the first will show its relationship with the general factor and the second with its allocated grouping factor.
# A key distinguishing feature between the bifactor model and the higher-order model is that the general factor is hypothesized to load directly on each of the observed variables.
isi.bi = "
sleep_diff =~ wakeup_early+staying_asleep+falling_asleep
dissatisf =~ satisfied+worried+wakeup_early
impact =~ worried+noticeable+inference
eta =~ inference+noticeable+worried+satisfied+falling_asleep+staying_asleep+wakeup_early
"
isi.bi_fit = cfa(model = isi.bi,data = Y,estimator="GLS")
summary(isi.bi_fit,standardize=TRUE)
# Bifactor structure
## More info at: https://www.frontiersin.org/articles/10.3389/fpsyg.2020.01357/full#h4
# The bifactor model (Holzinger and Swineford, 1937), also described as a nested-factor (NF) model (Gustafsson and Åberg-Bengtsson, 2010; Brunner et al., 2012), incorporates a general factor, which loads directly onto all of the observed variables in the model.
# One of the defining features of the bifactor model is that the grouping factors in the model are hypothesized to be orthogonal (uncorrelated) with the general factor.
# The bifactor model does not offer a “simple structure” solution in which each observed variable only loads onto a single factor (Gustafsson and Åberg-Bengtsson, 2010). Observed variables, by design, in this model load onto more than one factor, meaning that the variance explanation is split between (at least) two latencies. Each observed variable in the bifactor model is an indicator of both the general factor and one grouping factor. This means that each observed variable has two loading estimates in the model; the first will show its relationship with the general factor and the second with its allocated grouping factor.
# A key distinguishing feature between the bifactor model and the higher-order model is that the general factor is hypothesized to load directly on each of the observed variables.
isi.bi = "
sleep_diff =~ wakeup_early+staying_asleep+falling_asleep
dissatisf =~ satisfied+worried+wakeup_early
impact =~ worried+noticeable+inference
eta =~ inference+noticeable+worried+satisfied+falling_asleep+staying_asleep+wakeup_early
"
isi.bi_fit = cfa(model = isi.bi,data = Y,estimator="GLS")
summary(isi.bi_fit,standardize=TRUE)
fitmeasures(object = isi.twof_fit,fit.measures = c("chisq","df","pvalue","RMSEA","CFI","AIC"))
fitmeasures(object = isi.threef_fit,fit.measures = c("chisq","df",'npar',"pvalue","RMSEA","CFI"))
# (A) BFI: preparazione dati ----------------------------------------------
source("lab4.R") #recuperiamo il lavoro fatto nel precedente laboratorio
bfi = read.csv(file = "Datasets-20221124/bfi_reversed.csv",header = TRUE,sep = ",")
str(bfi)
head(bfi)
summary(bfi)
x11();par(mfrow=c(5,5))
source('Utilities-20221124/utilities.R')
out = split_dataset(data = bfi,prop = 0.50,seedx = 121)
bfi_A = out$A
bfi_B = out$B
# Iniziamo dalla matrice di correlazione osservata:
bfi.cor = cor(bfi_A[,1:25],method = "spearman")
corrplot::corrplot(bfi.cor, method = "color")
heatmap(x = bfi.cor,symm = TRUE,hclustfun = function(x){hclust(x,method="ward.D2")}) #visualizziamo la struttura di clustering
# Adattiamo lo stesso modello di clustering gerarchico alla matrice delle distanze (ottenute per trasformazione dalla matrice di correlazione):
bfi.hclust = hclust(d = dist(bfi.cor),method = "ward.D2")
plot(bfi.hclust) #dendrogramma
rect.hclust(tree=bfi.hclust,k = 3) # 3 gruppi individuati in precedenza
bfi.grp = cutree(bfi.hclust,k = 3) #raggruppamento degli items secondo hclust
print(bfi.grp)
#L'ispezione mediante clustering alla Ward dunque suggerisce il seguente raggruppamento (soluzione a tre cluster)
names(bfi.grp[bfi.grp==1]) # primo cluster
names(bfi.grp[bfi.grp==2]) # secondo cluster
names(bfi.grp[bfi.grp==3]) # terzo cluster
# Procediamo utilizzando un altro metodo per il clustering: il metodo del diametro massimo (complete linkage)
bfi.hclust = hclust(d = dist(bfi.cor),method = "complete")
plot(bfi.hclust) #dendrogramma
rect.hclust(tree=bfi.hclust,k = 4) # 4 gruppi individuati in precedenza
bfi.grp = cutree(bfi.hclust,k = 4) #raggruppamento degli items secondo hclust
# L'ispezione grafica suggerisce il seguente raggruppamento (soluzione a quattro gruppi):
names(bfi.grp[bfi.grp==1]) # primo cluster
names(bfi.grp[bfi.grp==2]) # secondo cluster
names(bfi.grp[bfi.grp==3]) # terzo cluster
names(bfi.grp[bfi.grp==4]) # quarto cluster
# L'ispezione grafica suggerisce il seguente raggruppamento (soluzione a tre gruppi):
rect.hclust(tree=bfi.hclust,k = 3,border = "blue")
bfi.grp = cutree(bfi.hclust,k = 3)
names(bfi.grp[bfi.grp==1]) # primo cluster
names(bfi.grp[bfi.grp==2]) # secondo cluster
names(bfi.grp[bfi.grp==3]) # terzo cluster
# (C) BFI: contenuto dgli items e soluzioni di clustering -----------------
# (C) BFI: contenuto dgli items e soluzioni di clustering -----------------
# Sulla base del contenuto degli items provare a comprendere se, da un punto di vista teorico, i raggruppamenti ottenuti mediante hclust
# Soluzione 1 (Ward method):
# Soluzione 1 (Ward method):
# "A1" "A2" "A3" "A4" "A5" "E1" "E2" "E3" "E4" "E5"
# Soluzione 1 (Ward method):
# "A1" "A2" "A3" "A4" "A5" "E1" "E2" "E3" "E4" "E5"
# "C1" "C2" "C3" "C4" "C5" "O1" "O2" "O3" "O4" "O5"
# Soluzione 2 (Complete method 1):
# Soluzione 2 (Complete method 1):
# "A1" "A2" "A3" "A4" "A5" "C1" "C2" "C3" "C4" "C5" "E1" "E2" "E4"
# Soluzione 2 (Complete method 1):
# "A1" "A2" "A3" "A4" "A5" "C1" "C2" "C3" "C4" "C5" "E1" "E2" "E4"
# "E3" "E5" "O1" "O3"
# Soluzione 2 (Complete method 1):
# "A1" "A2" "A3" "A4" "A5" "C1" "C2" "C3" "C4" "C5" "E1" "E2" "E4"
# "E3" "E5" "O1" "O3"
# "N1" "N2" "N3" "N4" "N5"
# Soluzione 3 (Complete method 2):
# Soluzione 3 (Complete method 2):
# "A1" "A2" "A3" "A4" "A5" "C1" "C2" "C3" "C4" "C5" "E1" "E2" "E3" "E4" "E5" "O1" "O3"
# Soluzione 3 (Complete method 2):
# "A1" "A2" "A3" "A4" "A5" "C1" "C2" "C3" "C4" "C5" "E1" "E2" "E3" "E4" "E5" "O1" "O3"
# "N1" "N2" "N3" "N4" "N5"
# Soluzione secondo la struttura semantica del BFI:
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# C1-C5
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# C1-C5
# E1-E5
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# C1-C5
# E1-E5
# N1-N5
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# C1-C5
# E1-E5
# N1-N5
# O1-O5
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# C1-C5
# E1-E5
# N1-N5
# O1-O5
# Soluzione secondo la struttura semantica del BFI:
# A1-A5
# C1-C5
# E1-E5
# N1-N5
# O1-O5
# Prima di definire il modello ed adattarlo ai dati, nel caso di variabili categoriali, occorre che queste siano definite come "ordered factors"
str(bfi_b)
# Facciamo un ciclo evitando di riscrivere a mano 25 volte la trasformazione:
bfi.ord = bfi_b # d'ora innanzi lavoriamo su bfi.ord che è lo stesso di bfi_B e contiene variabili dichiarate come ordinali
# Prima di definire il modello ed adattarlo ai dati, nel caso di variabili categoriali, occorre che queste siano definite come "ordered factors"
str(bfi_B)
# Facciamo un ciclo evitando di riscrivere a mano 25 volte la trasformazione:
bfi.ord = bfi_B # d'ora innanzi lavoriamo su bfi.ord che è lo stesso di bfi_B e contiene variabili dichiarate come ordinali
for(j in 1:25){
bfi.ord[,j] = factor(bfi.ord[,j],ordered = TRUE)
}
str(bfi.ord,1) #visualizziamo la nuova struttura
# Ora le variabili osservate sono tutte definite come categoriali ordinate (ordered factors). Trasformiamo anche le variabili "gender" e "education" in categoriali:
bfi.ord$gender = as.factor(bfi.ord$gender)
bfi.ord$education = as.factor(bfi.ord$education)
# modello teorico del bfi a 5 fattori
bfi.model0 = "piacevolez=~A1+A2+A3+A4+A5 \n coscienzios=~C1+C2+C3+C4+C5 \n estrovers=~E1+E2+E3+E4+E5 \n emozion=~N1+N2+N3+N4+N5 \n apertur=~O1+O2+O3+O4+O5"
# modello ottenuto via hclust tipo Ward
bfi.model1 = "f1=~N1+N2+N3+N4+N5 \n f2=~C1+C2+C3+C4+C5+O1+O2+O3+O4+O5 \n f3=~E1+E2+E3+E4+E5+A1+A2+A3+A4+A5"
# modello ottenuto via hclust tipo Complete linkage (prima soluzione)
bfi.model2a = "f1=~A1+A2+A3+A4+A5+C1+C2+C3+C4+C5+E1+E2+E4 \n f2=~E3+E5+O1+O3 \n f3=~N1+N2+N3+N4+N5 \n f4=~O2+O4+O5"
# modello ottenuto via hclust tipo Complete linkage (seconda soluzione)
bfi.model2b = "f1=~A1+A2+A3+A4+A5+C1+C2+C3+C4+C5+E1+E2+E3+E4+E5+O1+O3 \n f2=~N1+N2+N3+N4+N5 \n f3=~O2+O4+O5"
# stima dei modelli
bfi.cfa0 = cfa(model = bfi.model0, data = bfi.ord[,1:25], ordered = names(bfi.ord)[1:25], estimator="DWLS")
bfi.cfa1 = cfa(model = bfi.model1, data = bfi.ord[,1:25], ordered = names(bfi.ord)[1:25], estimator="DWLS")
bfi.cfa2a = cfa(model = bfi.model2a, data = bfi.ord[,1:25], ordered = names(bfi.ord)[1:25], estimator="DWLS")
bfi.cfa2b = cfa(model = bfi.model2b, data = bfi.ord[,1:25], ordered = names(bfi.ord)[1:25], estimator="DWLS")
# valutazione dell'adattamento ai dati (fit dei modelli) -- nota: AIC non disponibile quando estimator="DWLS"
bfi.fits = matrix(NA,4,5) #matrice per i risultati dei fit dei modelli
bfi.fits[1,] = fitmeasures(object = bfi.cfa0,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[2,] = fitmeasures(object = bfi.cfa1,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[3,] = fitmeasures(object = bfi.cfa2a,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
bfi.fits[4,] = fitmeasures(object = bfi.cfa2b,fit.measures = c("RMSEA","CFI","chisq","df","npar"))
colnames(bfi.fits) = c("RMSEA","CFI","chisq","df","npar")
rownames(bfi.fits) = c("model0","model1","model2a","model2b")
print(bfi.fits)
summary(bfi.cfa0,standardized=TRUE)
# Grafico di model0
#semPaths(bfi.cfa0,nCharNodes = 3,what = "model", whatLabels = "std.all",edge.label.cex = 1.5,edge.color = "black",sizeMan = 7,sizeLat=8,style = "lisrel",nDigits = 1,intercepts = FALSE,thresholds = FALSE)
plot_lavaan_model(fitted_model = bfi.cfa0)
# Estrazione delle matrici del modello
A = inspect(object = bfi.cfa0,what = "std.all")
A$lambda #Lambda
A$theta #Theta_delta
A$psi #Phi
# (C) BFI: analisi dei profili --------------------------------------------
# Calcolo dei punteggi fattoriali eta_hat (Bartlett)
bfi.cfa0 = cfa(model = bfi.model0,data = bfi.ord[,1:25],ordered = names(bfi.ord)[1:25],estimator="DWLS")
bfi.eta = lavPredict(object = bfi.cfa0,type = "lv",method = "regression")
# Calcoliamo i profili medi dei 5 fattori misurati rispetto al genere. Per facilitare il calcolo possiamo usare comodamente la funzione aggregate():
# medie:
bfi.aggreg.gender = aggregate(bfi.eta,list(bfi.ord$gender),mean)
bfi.aggreg.educ = aggregate(bfi.eta,list(bfi.ord$education),mean)
# Calcoliamo i profili medi dei 5 fattori misurati rispetto al genere. Per facilitare il calcolo possiamo usare comodamente la funzione aggregate():
# medie:
bfi.aggreg.gender = aggregate(bfi.eta,list(bfi.ord$gender),mean)
bfi.aggreg.educ = aggregate(bfi.eta,list(bfi.ord$education),mean)
# varianze:
bfi.aggreg.gender_var = aggregate(bfi.eta,list(bfi.ord$gender),var)
bfi.aggreg.educ_var = aggregate(bfi.eta,list(bfi.ord$education),var)
# Grafico 4x4 per i profili: in riga le variabili categoriali {gender, educ}, in colonna medie e varianze dei profili
x11(); par(mfrow=c(2,2))
